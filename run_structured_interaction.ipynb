{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import openai\n",
    "import schema\n",
    "import instructor\n",
    "from google import generativeai as gemini\n",
    "from prompts.patient_prompt import prompt as pp\n",
    "from prompts.doctor_prompt_structured import prompt as dp\n",
    "env_file = '.env'\n",
    "dotenv.load_dotenv(env_file, override=True)\n",
    "gemini.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "openai_api = openai.OpenAI(api_key=os.getenv(\"CORRELL_API_KEY\"))\n",
    "gemini_api = gemini.GenerativeModel(model_name='gemini-2.0-flash-lite')\n",
    "\n",
    "openai_client = instructor.from_openai(client=openai_api)\n",
    "gemini_client = instructor.from_gemini(client=gemini_api, mode=instructor.Mode.GEMINI_JSON)\n",
    "\n",
    "# Change me to test another model\n",
    "use_openai = True\n",
    "if use_openai:\n",
    "    unstructured_client = openai_api\n",
    "    client = openai_client\n",
    "    model = 'gpt-4o-mini'\n",
    "    call_doctor_kwargs = {\n",
    "        'model': model,\n",
    "        'temperature': 0.7,\n",
    "    }\n",
    "    call_patient_kwargs = {\n",
    "        'model': model,\n",
    "        'temperature': 0.7,\n",
    "    }\n",
    "else:\n",
    "    unstructured_client = gemini_api\n",
    "    client = gemini_client\n",
    "    model = 'gemini-2.0-flash-lite'\n",
    "    call_doctor_kwargs = {}\n",
    "    call_patient_kwargs = {}\n",
    "\n",
    "patient_profiles = pickle.load(open('patient_profiles.pkl', 'rb'))\n",
    "threshold = 0.8\n",
    "steps = 8\n",
    "num_profiles = 1 # how many interactions to run out of 240 profiles\n",
    "patient_profiles: dict[int, schema.Profile] = OrderedDict(itertools.islice(patient_profiles.items(), num_profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_doctor(messages: list[schema.Message]) -> schema.DoctorResponse:\n",
    "    response = client.completions.create(\n",
    "        # model=model,\n",
    "        messages=messages,\n",
    "        # temperature=0.7,\n",
    "        response_model=schema.DoctorResponse,\n",
    "        **call_doctor_kwargs\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def call_patient(messages: list[schema.Message]) -> str:\n",
    "    # response = unstructured_client.chat.completions.create(\n",
    "    #     model=model,\n",
    "    #     messages=messages,\n",
    "    #     temperature=0.7\n",
    "    # )\n",
    "    # return response.choices[0].message.content\n",
    "    response = client.completions.create(\n",
    "        messages=messages,\n",
    "        response_model=schema.PatientResponse,\n",
    "        **call_patient_kwargs\n",
    "    )\n",
    "    return response.response\n",
    "\n",
    "doctor_histories: dict[int, list[schema.Message]] = {}\n",
    "patient_histories: dict[int, list[schema.Message]] = {}\n",
    "\n",
    "# Iterate through patient profiles\n",
    "for i, profile in patient_profiles.items():\n",
    "    doctor_config = {\n",
    "        \"gender\": profile[\"gender\"],\n",
    "        \"ethnicity\": profile[\"ethnicity\"],\n",
    "        \"confidence_threshold\": threshold,\n",
    "        \"interaction_steps\": steps\n",
    "    }\n",
    "\n",
    "    # Initialize metadata\n",
    "    metadata = profile['interaction_metadata']\n",
    "    metadata.update({\n",
    "        \"diagnosis\": None,\n",
    "        \"diagnosis_success\": False,\n",
    "        \"interaction_duration\": 0,\n",
    "        \"num_symptoms_recovered\": 0,\n",
    "        \"confidence_history\": [],\n",
    "        'model': model,\n",
    "    })\n",
    "    \n",
    "    # Format prompts\n",
    "    pp_copy = pp.format(**profile)\n",
    "    dp_copy = dp.format(**doctor_config)\n",
    "    \n",
    "    # Initialize conversation histories\n",
    "    doctor_history: list[schema.Message] = [{\"role\": \"system\", \"content\": dp_copy}]\n",
    "    patient_history: list[schema.Message] = [{\"role\": \"system\", \"content\": pp_copy}]\n",
    "\n",
    "    doctor_reply = '<initial prompt>'\n",
    "\n",
    "    next_response_is_last = False\n",
    "    \n",
    "    # Run interaction loop\n",
    "    for step in range(steps):\n",
    "        metadata[\"interaction_duration\"] += 1\n",
    "        \n",
    "        # Update the patient history\n",
    "        patient_history.append({\"role\": \"user\", \"content\": doctor_reply})\n",
    "\n",
    "        # Patient response\n",
    "        patient_reply = call_patient(patient_history)\n",
    "\n",
    "        # Update patient conversation history\n",
    "        patient_history.append({\"role\": \"assistant\", \"content\": patient_reply})\n",
    "\n",
    "        # Update doctor conversation history\n",
    "        doctor_history.append({\"role\": \"user\", \"content\": patient_reply})\n",
    "\n",
    "        # Doctor response\n",
    "        doctor_response = call_doctor(doctor_history)\n",
    "        \n",
    "        # Update doctor conversation history with its full reply\n",
    "        doctor_history.append({\"role\": \"assistant\", \"content\": doctor_response.model_dump_json()})\n",
    "        \n",
    "        if len(doctor_response.diagnosis_rankings):\n",
    "            diagnosis = max(doctor_response.diagnosis_rankings,  key=lambda x: x.confidence)\n",
    "            metadata['diagnosis'] = diagnosis.diagnosis\n",
    "            metadata['confidence_history'].append(diagnosis.confidence)\n",
    "        # TODO: Counting could use improvement\n",
    "        metadata['num_symptoms_recovered'] += 1 if any(symptom in patient_reply for symptom in doctor_response.symptoms_to_verify_or_refute) else 0\n",
    "        \n",
    "        if doctor_response.diagnosis_relayed_to_patient:\n",
    "            metadata[\"diagnosis_success\"] = True\n",
    "            break\n",
    "\n",
    "        doctor_reply = doctor_response.response_to_patient\n",
    "    \n",
    "    # Store updated metadata\n",
    "    profile['interaction_metadata'] = metadata\n",
    "\n",
    "    # Store conversation histories\n",
    "    doctor_histories[i] = doctor_history\n",
    "    patient_histories[i] = patient_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation saved to conversations/0_6550.json\n"
     ]
    }
   ],
   "source": [
    "import save\n",
    "\n",
    "for patient_id in {0}:\n",
    "    filename = save.save_history(patient_profiles[patient_id], patient_histories[patient_id], doctor_histories[patient_id])\n",
    "    print(f'Conversation saved to {filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
