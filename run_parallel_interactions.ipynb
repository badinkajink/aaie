{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import openai\n",
    "import schema\n",
    "import save\n",
    "from prompts.patient_prompt import prompt as pp\n",
    "from prompts.doctor_prompt_structured import prompt as dp\n",
    "from prompts.symptom_check_prompt import prompt as scp\n",
    "from prompts.symptom_check_prompt import reply as scr\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from openai import OpenAIError, Timeout\n",
    "\n",
    "# Load environment variables\n",
    "env_file = \".env\"\n",
    "dotenv.load_dotenv(env_file, override=True)\n",
    "\n",
    "# OpenAI client setup\n",
    "client = openai.OpenAI(api_key=os.getenv(\"CORRELL_API_KEY\"))\n",
    "\n",
    "# Load patient profiles\n",
    "patient_profiles = pickle.load(open(\"patient_profiles.pkl\", \"rb\"))\n",
    "threshold = 0.8\n",
    "steps = 5\n",
    "\n",
    "# Total number of profiles to process\n",
    "num_profiles = len(patient_profiles)  # 240 profiles\n",
    "batch_size = 10  # 10 profiles per process\n",
    "num_workers = num_profiles // batch_size  # 24 parallel processes\n",
    "\n",
    "# Convert to an ordered dictionary\n",
    "patient_profiles: dict[int, schema.Profile] = OrderedDict(patient_profiles)\n",
    "\n",
    "# Split into batches of 10 profiles\n",
    "profile_batches = [\n",
    "    dict(itertools.islice(patient_profiles.items(), i * batch_size, (i + 1) * batch_size))\n",
    "    for i in range(num_workers)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry logic for API calls\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),  # Retry up to 5 times\n",
    "    wait=wait_exponential(min=1, max=10),  # Exponential backoff between retries (1s to 10s)\n",
    "    retry=retry_if_exception_type((Timeout, OpenAIError))  # Retry on timeout or OpenAIError\n",
    ")\n",
    "def call_openai_doctor(messages: list[schema.Message]) -> schema.DoctorResponse:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        response_format=schema.DoctorResponse\n",
    "    )\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(min=1, max=10),\n",
    "    retry=retry_if_exception_type((Timeout, OpenAIError))\n",
    ")\n",
    "def call_openai(messages: list[schema.Message]) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(min=1, max=10),\n",
    "    retry=retry_if_exception_type((Timeout, OpenAIError))\n",
    ")\n",
    "def call_openai_symptom_check(messages: list[schema.Message]) -> schema.SymptomCheck:\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.4,\n",
    "        response_format=schema.SymptomCheck\n",
    "    )\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "# Diagnosis confidence formatting\n",
    "def get_diagnosis_confidence(diagnosis_history: list[str, float]) -> dict[str, float]:\n",
    "    all_diagnoses = {d.diagnosis for step in diagnosis_history for d in step}\n",
    "    diagnosis_confidence = {d: [] for d in all_diagnoses}\n",
    "    \n",
    "    for step in diagnosis_history:\n",
    "        step_conf_dict = {d.diagnosis: d.confidence for d in step}\n",
    "        for d in all_diagnoses:\n",
    "            diagnosis_confidence[d].append(step_conf_dict.get(d, np.nan))  # Use NaN if missing\n",
    "\n",
    "    return diagnosis_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a batch of profiles\n",
    "def process_profiles(profiles_batch: dict[int, schema.Profile], batch_id: int):\n",
    "    print(f\"Starting batch {batch_id} with {len(profiles_batch)} profiles...\")\n",
    "\n",
    "    for i, profile in profiles_batch.items():\n",
    "        patient_data = {k: profile[k] for k in list(OrderedDict(profile))[1:-1]}\n",
    "        doctor_config = {\n",
    "            \"gender\": patient_data[\"gender\"],\n",
    "            \"ethnicity\": patient_data[\"ethnicity\"],\n",
    "            \"confidence_threshold\": threshold,\n",
    "            \"interaction_steps\": steps\n",
    "        }\n",
    "\n",
    "        # Initialize metadata\n",
    "        metadata = profile[\"interaction_metadata\"]\n",
    "        metadata.update({\n",
    "            \"diagnosis\": None,\n",
    "            \"diagnosis_success\": False,\n",
    "            \"interaction_duration\": 0,\n",
    "            \"num_symptoms_recovered\": 0,\n",
    "            \"confidence_history\": []\n",
    "        })\n",
    "\n",
    "        # Format prompts\n",
    "        pp_copy = pp.format(**patient_data)\n",
    "        dp_copy = dp.format(**doctor_config)\n",
    "\n",
    "        # Initialize conversation histories\n",
    "        doctor_history = [{\"role\": \"system\", \"content\": dp_copy}]\n",
    "        patient_history = [{\"role\": \"system\", \"content\": pp_copy}]\n",
    "        patient_reply = \"\"\n",
    "\n",
    "        next_response_is_last = False\n",
    "        doctor_responses = []\n",
    "        print(f\"Processing profile {i + 1}/{num_profiles}...\")\n",
    "\n",
    "        # Run interaction loop\n",
    "        for step in range(steps):\n",
    "            if step == 0:\n",
    "                greeting = \"Hi, I'll be your doctor today. What brings you in?\"\n",
    "                patient_history.append({\"role\": \"assistant\", \"content\": greeting})\n",
    "                patient_reply = call_openai(patient_history)\n",
    "                patient_history.append({\"role\": \"user\", \"content\": patient_reply})\n",
    "\n",
    "            metadata[\"interaction_duration\"] += 1\n",
    "            doctor_history.append({\"role\": \"user\", \"content\": patient_reply})\n",
    "\n",
    "            # Doctor response\n",
    "            doctor_response = call_openai_doctor(doctor_history)\n",
    "            doctor_responses.append(doctor_response)\n",
    "\n",
    "            # Update doctor conversation history\n",
    "            doctor_history.append({\"role\": \"assistant\", \"content\": doctor_response.model_dump_json()})\n",
    "\n",
    "            if doctor_response.diagnosis_rankings:\n",
    "                diagnosis = max(doctor_response.diagnosis_rankings, key=lambda x: x.confidence)\n",
    "                metadata[\"diagnosis\"] = diagnosis.diagnosis\n",
    "                metadata[\"confidence_history\"].append(diagnosis.confidence)\n",
    "\n",
    "            if next_response_is_last:\n",
    "                break\n",
    "            elif metadata[\"confidence_history\"] and metadata[\"confidence_history\"][-1] >= threshold:\n",
    "                metadata[\"diagnosis_success\"] = \"melanoma\" in metadata[\"diagnosis\"].lower()\n",
    "                next_response_is_last = True\n",
    "\n",
    "            patient_history.append({\"role\": \"user\", \"content\": doctor_response.response_to_patient})\n",
    "            patient_reply = call_openai(patient_history + [{\"role\": \"user\", \"content\": doctor_response.response_to_patient}])\n",
    "            patient_history.append({\"role\": \"assistant\", \"content\": patient_reply})\n",
    "\n",
    "        recovered_symptoms_history = [d.known_symptoms for d in doctor_responses]\n",
    "        symptoms = {**patient_data[\"revealed_symptoms\"], **patient_data[\"hidden_symptoms\"]}\n",
    "        scr_copy = scr.format(recovered_symptoms_history=recovered_symptoms_history, symptoms=symptoms)\n",
    "        symptom_check = [{\"role\": \"system\", \"content\": scp}, {\"role\": \"user\", \"content\": scr_copy}]\n",
    "        symptom_check_response = call_openai_symptom_check(symptom_check)\n",
    "\n",
    "        metadata[\"num_symptoms_recovered\"] = symptom_check_response.found_symptoms\n",
    "        metadata[\"num_symptoms_recovered_history\"] = symptom_check_response.found_symptoms_history\n",
    "\n",
    "        diagnosis_history = [d.diagnosis_rankings for d in doctor_responses]\n",
    "        metadata[\"diagnosis_confidence_history\"] = get_diagnosis_confidence(diagnosis_history)\n",
    "\n",
    "        profile[\"interaction_metadata\"] = metadata\n",
    "        save.save_history(profile, patient_history, doctor_history)\n",
    "\n",
    "    print(f\"Batch {batch_id} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel execution\n",
    "with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "    for batch_id, batch in enumerate(profile_batches):\n",
    "        executor.submit(process_profiles, batch, batch_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
